---
layout: post
title: scraping
---

* step 1  

  find a link to download full information of one issue, not only one article:  
  
  * ``http://www.perseus.tufts.edu/hopper/dltext?doc=Perseus%3atext%3a2006.05.0001``  
  
--------------

* step 2  
  download html version of the index site with  
  
  * ``wget http://www.perseus.tufts.edu/hopper/collection?collection=Perseus:collection:RichTimes``  

--------------

* step 3  
  open in sublime text and find a regular expression like this to get all the links to the different issues:  
  
  * ``text\?doc.+\.(\d\d\d\d)``  
  
copy paste links into a new file  
  
--------------

* step 4  
  complete list of links with the link found in step 1.  
  
* find: ``^te``  
  replace: ``http://www.perseus.tufts.edu/hopper/dlte``  

--------------

* step 5  
  save as a .txt file  

--------------

* step 6  
  download all the information with wget using  
  
  * ``wget -i links.txt -P ./folderYouWantToSaveTo/``
